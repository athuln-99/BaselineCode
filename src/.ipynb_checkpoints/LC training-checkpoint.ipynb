{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e0b82eb7",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\athul raj nambiar\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\scipy\\__init__.py:173: UserWarning: A NumPy version >=1.19.5 and <1.27.0 is required for this version of SciPy (detected version 1.18.1)\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import torch\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import DataLoader\n",
    "import transformers\n",
    "from transformers import RobertaTokenizer, RobertaForMaskedLM, RobertaModel\n",
    "from transformers import BertTokenizer\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "from torch.optim import Adam\n",
    "import time\n",
    "import json\n",
    "\n",
    "import random\n",
    "from collections import Counter, defaultdict\n",
    "import numpy as np\n",
    "from sample_few_shot import get_label_dict\n",
    "from finetune_model import RobertaNER, BertNER\n",
    "from eval_util import batch_span_eval\n",
    "from data import *\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "df83296b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is available\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    print(\"CUDA is available\")\n",
    "else:\n",
    "    print(\"CUDA is not available\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e95ac107",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_batch(batch):\n",
    "    text = [F.pad(torch.tensor(x[0]), (0,max_seq_len-len(x[0])), \"constant\", 1) for x in batch] # batch_size * max_seq_len \n",
    "    text = pad_sequence(text, batch_first = True)\n",
    "    attention_mask = [torch.cat((torch.ones_like(torch.tensor(x[0])), torch.zeros(max_seq_len-len(x[0]), dtype=torch.int64)), dim=0)\n",
    "        if len(x[0]) < max_seq_len else torch.ones_like(torch.tensor(x[0]))[:max_seq_len] for x in batch]\n",
    "    attention_mask = pad_sequence(attention_mask, batch_first = True)\n",
    "    label = [F.pad(torch.tensor(x[1]), (0,max_seq_len-len(x[1])), \"constant\", -100) for x in batch]\n",
    "    label = pad_sequence(label, batch_first = True)\n",
    "    orig_len = [len(x[0]) for x in batch]\n",
    "\n",
    "    return text, attention_mask, label, orig_len\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d1b94753",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_func(processed_training_set, epoch, tokenizer, label_sentence_dicts, soft_kmeans, count_num = 0, unsup_data_iter = None):\n",
    "\n",
    "    # Train the model\n",
    "    # print(\"learning rate: \")\n",
    "    train_loss = 0\n",
    "    total_pred, total_gold, total_crct = 0.0, 0.0, 0.0\n",
    "    dataset_chosen = []\n",
    "    data = []\n",
    "    for i,d in enumerate(processed_training_set):\n",
    "        one_dataset = DataLoader(d, batch_size=BATCH_SIZE, collate_fn=generate_batch)\n",
    "        data.extend(one_dataset)\n",
    "        dataset_chosen.extend([i for x in range(len(one_dataset))])\n",
    "    all_data_index = [i for i in range(len(dataset_chosen))]\n",
    "    print(\"shuffling sentences\")\n",
    "    random.shuffle(all_data_index)\n",
    "    # print(\"first 50 data:\")\n",
    "    # print(all_data_index[:50])\n",
    "    \n",
    "    model.train()\n",
    "    print(f\"total {len(all_data_index)} iters\")\n",
    "    for k in all_data_index:\n",
    "        count_num += 1\n",
    "        text, attention_mask, cls, orig_len = data[k]\n",
    "        id2label = id2labels[dataset_chosen[k]]\n",
    "        optimizer.zero_grad()\n",
    "        outputs = []\n",
    "        text_1, attention_mask_1, cls_1 = text.to(device), attention_mask.to(device), cls.to(device)\n",
    "        cls_2 = cls.to(device)\n",
    "        # for p,sent in enumerate(cls_2):\n",
    "        #     for q,cl in enumerate(sent):\n",
    "        #         if cl == 0 and np.random.random_sample() > 0.2:\n",
    "        #             cls_2[p][q]=-100\n",
    "        # print(cls_1[0])\n",
    "        # print(cls_2[0])\n",
    "        loss, output = model(text_1, attention_mask=attention_mask_1, labels=cls_2, dataset = dataset_chosen[k])\n",
    "        loss.mean().backward()\n",
    "        train_loss += loss.mean().item()\n",
    "        outputs=output\n",
    "        optimizer.step()\n",
    "        preds = [[id2label[int(x)] for j,x in enumerate(y[1:orig_len[i]-1]) if int(cls[i][j + 1]) != -100] for i,y in enumerate(outputs)]\n",
    "        gold = [[id2label[int(x)] for x in y[1:orig_len[i]-1] if int(x) != -100] for i,y in enumerate(cls)]\n",
    "    \n",
    "        bpred, bgold, bcrct, _, _, _ = batch_span_eval(preds, gold)\n",
    "        total_pred += bpred\n",
    "        total_gold += bgold\n",
    "        total_crct += bcrct\n",
    "\n",
    "        \n",
    "        if count_num%200 == 0:\n",
    "            print(f\"batch: {count_num}/{int(num_training_steps/N_EPOCHS)} lr: {optimizer.param_groups[0]['lr']:.9f} loss: {loss.mean().item()/BATCH_SIZE:.9f}\")\n",
    "            \n",
    "        # Adjust the learning rate\n",
    "        scheduler.step()\n",
    "\n",
    "        if unsup_data_iter is not None:\n",
    "            # print(f\"unsup batches in this step: {int(unsup_batch_num / len(all_data_index)  * (k + 1)) - int(unsup_batch_num / len(all_data_index) * k)}\")\n",
    "            for k1 in range(int(unsup_batch_num / len(all_data_index) * k), int(unsup_batch_num / len(all_data_index) * (k + 1))):\n",
    "                text, attention_mask, t_prob, orig_len = next(unsup_data_iter)\n",
    "                optimizer.zero_grad()\n",
    "                text_1, attention_mask_1, t_prob1 = text.to(device), attention_mask.to(device), t_prob.to(device)\n",
    "                loss, output = model.forward_unsup(text_1, attention_mask=attention_mask_1, dataset = 0, t_prob = t_prob1)\n",
    "                loss = args.unsup_lr * loss\n",
    "                loss.mean().backward()\n",
    "                train_loss += loss.mean().item()\n",
    "                outputs=output\n",
    "                optimizer.step()\n",
    "                scheduler.step()\n",
    "            # for k1, (text, attention_mask, t_prob, orig_len) in enumerate(data_loader[unsup_batch_num / len(all_data_index) / N_EPOCHS * (epoch * len(all_data_index) + k ):unsup_batch_num / len(all_data_index) / N_EPOCHS * (epoch * len(all_data_index) + k + 1)]): \n",
    "            #     print(k1)\n",
    "\n",
    "\n",
    "    microp = total_crct/total_pred if total_pred > 0 else 0\n",
    "    micror = total_crct/total_gold if total_gold > 0 else 0\n",
    "    microf1 = 2*microp*micror/(microp + micror) if (microp + micror) > 0 else 0\n",
    "\n",
    "    return train_loss / train_num_data_point * BATCH_SIZE, microp, micror, microf1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c5348f14",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(data_, epoch, label_sentence_dicts, soft_kmeans, test_all_data = True, finetune = False):\n",
    "    val_loss = 0\n",
    "    total_pred, total_gold, total_crct = 0.0, 0.0, 0.0\n",
    "    total_pred_per_type, total_gold_per_type, total_crct_per_type = defaultdict(int), defaultdict(int), defaultdict(int)\n",
    "    # data = DataLoader(data_, batch_size=256, collate_fn=generate_batch)\n",
    "    # data = generate_episode(data_, epoch,  EPISODE_NUM, TEST_SUP_CLS_NUM, test_id2label, label_sentence_dict)\n",
    "    if test_all_data:\n",
    "        dataset_chosen = []\n",
    "        data = []\n",
    "        for i,d in enumerate(data_):\n",
    "            one_dataset = DataLoader(d, batch_size=64, collate_fn=generate_batch)\n",
    "            data.extend(one_dataset)\n",
    "            dataset_chosen.extend([i for x in range(len(one_dataset))])\n",
    "        # data = generate_testing_episode(data_, epoch+1, TEST_SUP_CLS_NUM, test_id2label, label_sentence_dict)\n",
    "    else:\n",
    "        data, _ = generate_episode(data_, epoch,  EPISODE_NUM, TEST_SUP_CLS_NUM, test_id2label, label_sentence_dict, use_multipledata = False)\n",
    "    idx = 0\n",
    "    f1ss = []\n",
    "    pss = []\n",
    "    rss = []\n",
    "    # device = torch.device('cuda:0')\n",
    "    new_model = model\n",
    "    new_model.eval()\n",
    "    for j, (text, attention_mask, cls, orig_len) in enumerate(data): \n",
    "        id2label = id2labels[dataset_chosen[j]]\n",
    "        with torch.no_grad():\n",
    "            text_1, attention_mask_1, cls_1 = text.to(device), attention_mask.to(device).to(device), cls.to(device)\n",
    "            # we use the same dataset for training and testing\n",
    "            loss, outputs = new_model(text_1, attention_mask=attention_mask_1, labels=cls_1, dataset = dataset_chosen[j])\n",
    "            val_loss += loss.mean().item()\n",
    "        preds = [[id2label[int(x)] for j,x in enumerate(y[1:orig_len[i]-1]) if int(cls[i][j + 1]) != -100] for i,y in enumerate(outputs)]\n",
    "        gold = [[id2label[int(x)] for x in y[1:orig_len[i]-1] if int(x) != -100] for i,y in enumerate(cls)]\n",
    "        \n",
    "        \n",
    "        for pred in preds:\n",
    "            for t,token in enumerate(pred):\n",
    "                if len(token.split('I-')) == 2:\n",
    "                    if t == 0:\n",
    "                        pred[t] = 'O'\n",
    "                        continue\n",
    "                    else:\n",
    "                        tag = token.split('I-')[1]\n",
    "                        if len(pred[t-1]) == 1:\n",
    "                            pred[t] = 'O'\n",
    "                        else:\n",
    "                            if tag != pred[t-1].split('-')[1]:\n",
    "                                  pred[t] = 'O'  \n",
    "\n",
    "        bpred, bgold, bcrct, pred_span_per_type, gold_span_per_type, crct_span_per_type = batch_span_eval(preds, gold)\n",
    "        total_pred += bpred\n",
    "        total_gold += bgold\n",
    "        total_crct += bcrct\n",
    "        \n",
    "        for x in pred_span_per_type:\n",
    "            total_pred_per_type[x] += pred_span_per_type[x]\n",
    "            total_gold_per_type[x] += gold_span_per_type[x]\n",
    "            total_crct_per_type[x] += crct_span_per_type[x]\n",
    "\n",
    "    microp = total_crct/total_pred if total_pred > 0 else 0\n",
    "    micror = total_crct/total_gold if total_gold > 0 else 0\n",
    "    microf1 = 2*microp*micror/(microp + micror) if (microp + micror) > 0 else 0\n",
    "    microp_per_type, micror_per_type, microf1_per_type = {}, {}, {}\n",
    "    for x in total_pred_per_type:\n",
    "        microp_per_type[x] = total_crct_per_type[x]/total_pred_per_type[x] if total_pred_per_type[x] > 0 else 0\n",
    "        micror_per_type[x] = total_crct_per_type[x]/total_gold_per_type[x] if total_gold_per_type[x] > 0 else 0\n",
    "        microf1_per_type[x] = 2*microp_per_type[x]*micror_per_type[x]/(microp_per_type[x]+micror_per_type[x]) if (microp_per_type[x]+micror_per_type[x]) > 0 else 0\n",
    "\n",
    "    return val_loss / val_num_data_point * 64, microp, micror, microf1, microp_per_type, micror_per_type, microf1_per_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "039c3f0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prob(data_, id2labels):\n",
    "    val_loss = 0\n",
    "    total_pred, total_gold, total_crct = 0.0, 0.0, 0.0\n",
    "    total_pred_per_type, total_gold_per_type, total_crct_per_type = defaultdict(int), defaultdict(int), defaultdict(int)\n",
    "    # data = DataLoader(data_, batch_size=256, collate_fn=generate_batch)\n",
    "    # data = generate_episode(data_, epoch,  EPISODE_NUM, TEST_SUP_CLS_NUM, test_id2label, label_sentence_dict)\n",
    "\n",
    "    dataset_chosen = []\n",
    "    data = []\n",
    "    for i,d in enumerate(data_):\n",
    "        one_dataset = DataLoader(d, batch_size=64, collate_fn=generate_batch)\n",
    "        data.extend(one_dataset)\n",
    "        dataset_chosen.extend([i for x in range(len(one_dataset))])\n",
    "    prob = []\n",
    "\n",
    "    model.eval()\n",
    "    for j, (text, attention_mask, cls, orig_len) in enumerate(data): \n",
    "        id2label = id2labels[dataset_chosen[j]]\n",
    "        with torch.no_grad():\n",
    "            text_1, attention_mask_1, cls_1 = text.to(device), attention_mask.to(device).to(device), cls.to(device)\n",
    "            # we use the same dataset for training and testing\n",
    "            loss, output, logits = model(text_1, attention_mask=attention_mask_1, labels=cls_1, dataset = dataset_chosen[j], output_logits=True)\n",
    "            val_loss += loss.mean().item()\n",
    "            prob.extend(logits)\n",
    "        preds = [[id2label[int(x)] for j,x in enumerate(y[1:orig_len[i]-1]) if int(cls[i][j + 1]) != -100] for i,y in enumerate(output)]\n",
    "        gold = [[id2label[int(x)] for x in y[1:orig_len[i]-1] if int(x) != -100] for i,y in enumerate(cls)]\n",
    "\n",
    "        bpred, bgold, bcrct, pred_span_per_type, gold_span_per_type, crct_span_per_type = batch_span_eval(preds, gold)\n",
    "        total_pred += bpred\n",
    "        total_gold += bgold\n",
    "        total_crct += bcrct\n",
    "        for x in pred_span_per_type:\n",
    "            total_pred_per_type[x] += pred_span_per_type[x]\n",
    "            total_gold_per_type[x] += gold_span_per_type[x]\n",
    "            total_crct_per_type[x] += crct_span_per_type[x]\n",
    "\n",
    "    microp = total_crct/total_pred if total_pred > 0 else 0\n",
    "    micror = total_crct/total_gold if total_gold > 0 else 0\n",
    "    microf1 = 2*microp*micror/(microp + micror) if (microp + micror) > 0 else 0\n",
    "          \n",
    "    prob = pad_sequence(prob, batch_first = True)\n",
    "    print(\"predicted probablity shape\")\n",
    "    print(prob.shape)\n",
    "    microp = total_crct/total_pred if total_pred > 0 else 0\n",
    "    micror = total_crct/total_gold if total_gold > 0 else 0\n",
    "    microf1 = 2*microp*micror/(microp + micror) if (microp + micror) > 0 else 0\n",
    "    print(f'\\tPrec: {microp * 100:.1f}%(val)\\t|\\tRecall: {micror * 100:.1f}%(val)\\t|\\tF1: {microf1 * 100:.1f}%(val)')\n",
    "    \n",
    "    return prob\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "36dd6274",
   "metadata": {},
   "outputs": [],
   "source": [
    "def str2bool(v):\n",
    "    if isinstance(v, bool):\n",
    "       return v\n",
    "    if v.lower() in ('yes', 'true', 't', 'y', '1'):\n",
    "        return True\n",
    "    elif v.lower() in ('no', 'false', 'f', 'n', '0'):\n",
    "        return False\n",
    "    else:\n",
    "        raise argparse.ArgumentTypeError('Boolean value expected.')\n",
    "\n",
    "def weights_init_custom(model):\n",
    "    init_layers = [9, 10, 11]\n",
    "    dense_names = [\"query\", \"key\", \"value\", \"dense\"]\n",
    "    layernorm_names = [\"LayerNorm\"]\n",
    "    for name, module in model.bert.named_parameters():\n",
    "        if any(f\".{i}.\" in name for i in init_layers):\n",
    "            if any(n in name for n in dense_names):\n",
    "                if \"bias\" in name:\n",
    "                    module.data.zero_()\n",
    "                elif \"weight\" in name:\n",
    "                    module.data.normal_(mean=0.0, std=model.config.initializer_range)\n",
    "            elif any(n in name for n in layernorm_names):\n",
    "                if \"bias\" in name:\n",
    "                    module.data.zero_()\n",
    "                elif \"weight\" in name:\n",
    "                    module.data.fill_(1.0)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9da4c3b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Args:\n",
    "    def __init__(self):\n",
    "        i = 0\n",
    "        self.datapath = 'dataset'\n",
    "        self.dataset = 'custom'\n",
    "        self.train_text = f'FS_train_dataset{i}.words'\n",
    "        self.train_ner = f'FS_train_dataset{i}.ner'\n",
    "        self.test_text = f'FS_test_dataset{i}.words'\n",
    "        self.test_ner = f'FS_test_dataset{i}.ner'\n",
    "        self.model_save_name = f'FS_train_dataset{i}_finetuned_model'\n",
    "        self.few_shot_sets = 1\n",
    "        self.unsup_text = None\n",
    "        self.unsup_ner = None\n",
    "        self.base_model = 'roberta'\n",
    "        self.epoch = 5\n",
    "        self.train_cls_num = 4\n",
    "        self.test_cls_num = 18\n",
    "        self.max_seq_len = 128\n",
    "        self.batch_size = 8\n",
    "        self.soft_kmeans = False\n",
    "        self.lr = 1e-04\n",
    "        self.unsup_lr = 0.5\n",
    "        self.warmup_proportion = 0.1\n",
    "        self.weight_decay = 0.01\n",
    "        self.use_truecase = False\n",
    "        self.local_rank = None\n",
    "        self.use_gpu = 'cuda'\n",
    "        self.data_size = ''\n",
    "        self.load_model = True\n",
    "        self.reinit = False\n",
    "        self.load_model_name = 'pretrained_models/lc_pretrained_190.pt'\n",
    "        self.load_checkpoint = False\n",
    "        self.load_dataset = False\n",
    "        self.train_dataset_file = None\n",
    "        self.test_dataset_file = None\n",
    "        self.label2ids = None\n",
    "        self.id2labels = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "78909626",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = Args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "483de66c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# if __name__ == \"__main__\":\n",
    "\n",
    "#     total_f1_scores = []\n",
    "#     train_text_file = args.train_text\n",
    "#     train_ner_file = args.train_ner\n",
    "#     for round in range(args.few_shot_sets):\n",
    "#         if '.' not in args.train_text:\n",
    "#             args.train_text = train_text_file + f'_{round}.words'\n",
    "#             args.train_ner = train_ner_file + f'_{round}.ner'\n",
    "#         print(f\"train text is {args.train_text}\")\n",
    "#         datasets = args.dataset.split('_')\n",
    "#         print(datasets)\n",
    "#         train_texts = [os.path.join(args.datapath, dataset, args.train_text) for dataset in datasets]\n",
    "#         train_ners = [os.path.join(args.datapath, dataset, args.train_ner) for dataset in datasets]\n",
    "#         test_texts = [os.path.join(args.datapath, dataset, args.test_text) for dataset in datasets]\n",
    "#         test_ners = [os.path.join(args.datapath, dataset, args.test_ner) for dataset in datasets]\n",
    "\n",
    "#         max_seq_len = args.max_seq_len\n",
    "#         base_model = args.base_model\n",
    "\n",
    "#         if base_model == 'roberta':\n",
    "#             tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "#         elif base_model == 'bert':\n",
    "#             tokenizer = BertTokenizer.from_pretrained('bert-base-cased')\n",
    "\n",
    "#         label2ids, id2labels = [], []\n",
    "#         processed_training_set, train_label_sentence_dicts, processed_val_set, val_label_sentence_dicts = [], [], [], []\n",
    "#         if not args.load_dataset:\n",
    "#             for train_text, train_ner, test_text, test_ner in zip(train_texts, train_ners, test_texts, test_ners):\n",
    "#                 with open(train_ner, encoding='utf-8') as fner, open(train_text, encoding='utf-8') as f:\n",
    "#                     train_ner_tags, train_words = fner.readlines(), f.readlines()\n",
    "#                 with open(test_ner, encoding='utf-8') as fner, open(test_text, encoding='utf-8') as f:\n",
    "#                     test_ner_tags, test_words = fner.readlines(), f.readlines()   \n",
    "                \n",
    "#                 # Taking the first 20% of test set for validation set\n",
    "#                 split_index = int(len(test_ner_tags) * 0.2)\n",
    "                \n",
    "#                 val_ner_tags = test_ner_tags[:split_index]\n",
    "#                 val_words = test_words[:split_index]\n",
    "                \n",
    "                \n",
    "# #                 # Creating a validation set\n",
    "# #                 validation_data_size = int(0.05 * len(train_ner_tags))  # 5% of the data size\n",
    "# #                 indexes = list(range(len(train_ner_tags)))  # Create a list of indexes\n",
    "\n",
    "# #                 random.shuffle(indexes)  # Shuffle the list of indexes\n",
    "\n",
    "# #                 validation_indexes = indexes[:validation_data_size]  # Take the first 5% as validation indexes\n",
    "# #                 training_indexes = indexes[validation_data_size:]  # The rest will be used for training\n",
    "\n",
    "# #                 # Now you can create the validation and training data based on the selected indexes\n",
    "# #                 val_ner_tags = [train_ner_tags[i] for i in validation_indexes]\n",
    "# #                 val_words = [train_words[i] for i in validation_indexes]\n",
    "# #                 train_ner_tags = [train_ner_tags[i] for i in training_indexes]\n",
    "# #                 train_words = [train_words[i] for i in training_indexes]\n",
    "                \n",
    "#                 label2id, id2label = get_label_dict([train_ner_tags, val_ner_tags])\n",
    "                \n",
    "#                 label2ids.append(label2id)\n",
    "#                 id2labels.append(id2label)\n",
    "\n",
    "#                 train_ner_tags, train_words, train_label_sentence_dict = process_data(train_ner_tags, train_words, tokenizer, label2id, max_seq_len,base_model=base_model,use_truecase=args.use_truecase)\n",
    "#                 val_ner_tags, val_words, val_label_sentence_dict = process_data(val_ner_tags, val_words, tokenizer, label2id, max_seq_len,base_model=base_model,use_truecase=args.use_truecase)\n",
    "\n",
    "#                 sub_train_ = [[train_words[i], train_ner_tags[i]] for i in range(len(train_ner_tags))]\n",
    "#                 sub_valid_ = [[val_words[i], val_ner_tags[i]] for i in range(len(val_ner_tags))] \n",
    "\n",
    "#                 train_label_sentence_dicts.append(train_label_sentence_dict)\n",
    "#                 val_label_sentence_dicts.append(val_label_sentence_dict)\n",
    "\n",
    "#                 processed_training_set.append(sub_train_) \n",
    "#                 processed_val_set.append(sub_valid_) \n",
    "\n",
    "            \n",
    "\n",
    "#         dataset_label_nums = [len(x) for x in label2ids]\n",
    "#         print(f\"dataset label nums: {dataset_label_nums}\")\n",
    "#         train_num_data_point = sum([len(sub_train_) for sub_train_ in processed_training_set])\n",
    "#         print(f\"number of all training data points: {train_num_data_point}\")\n",
    "#         val_num_data_point = sum([len(sub_train_) for sub_train_ in processed_val_set])\n",
    "#         print(f\"number of all testing data points: {val_num_data_point}\")\n",
    "\n",
    "\n",
    "\n",
    "#         LOAD_MODEL = args.load_model\n",
    "#         if LOAD_MODEL:\n",
    "#             if 'checkpoint' not in args.load_model_name:\n",
    "#                 state = torch.load(args.load_model_name)\n",
    "#                 if base_model == 'roberta':\n",
    "#                     model = RobertaNER.from_pretrained('roberta-base', dataset_label_nums = dataset_label_nums, output_attentions=False, output_hidden_states=False, multi_gpus=True)\n",
    "#                 elif base_model == 'bert':\n",
    "#                     model = BertNER.from_pretrained('bert-base-cased', dataset_label_nums = dataset_label_nums, output_attentions=False, output_hidden_states=False, multi_gpus=True)\n",
    "#                 pretrained_dict = state.state_dict()\n",
    "#                 model_dict = model.state_dict()\n",
    "#                 pretrained_dict = {k:v for k,v in pretrained_dict.items() if k in model_dict and 'classifiers.0.' not in k} \n",
    "#                 model_dict.update(pretrained_dict)\n",
    "#                 model.load_state_dict(model_dict)\n",
    "#                 model.dataset_label_nums = dataset_label_nums\n",
    "#             else:\n",
    "#                 state = torch.load(args.load_model_name)\n",
    "#                 if base_model == 'roberta':\n",
    "#                     model = RobertaNER.from_pretrained('roberta-base', dataset_label_nums = dataset_label_nums, output_attentions=False, output_hidden_states=False, multi_gpus=True)\n",
    "#                 elif base_model == 'bert':\n",
    "#                     model = BertNER.from_pretrained('bert-base-cased', dataset_label_nums = dataset_label_nums, output_attentions=False, output_hidden_states=False, multi_gpus=True)\n",
    "#                 pretrained_dict = state['state_dict']\n",
    "#                 model_dict = model.state_dict()\n",
    "#                 pretrained_dict = {k.split('module.')[1]:v for k,v in pretrained_dict.items() if k.split('module.')[1] in model_dict and 'classifier' not in k}\n",
    "#                 print(\"pretrained dict\")\n",
    "#                 print(pretrained_dict)\n",
    "#                 model_dict.update(pretrained_dict)\n",
    "#                 model.load_state_dict(model_dict)\n",
    "#                 # model.load_state_dict(state['state_dict'])\n",
    "#                 print(model.config.hidden_size)\n",
    "#             model.classifiers = torch.nn.ModuleList([torch.nn.Linear(model.config.hidden_size, x) for x in dataset_label_nums])\n",
    "#         else:\n",
    "#             if base_model == 'roberta':\n",
    "#                 model = RobertaNER.from_pretrained('roberta-base', dataset_label_nums = dataset_label_nums, output_attentions=False, output_hidden_states=False, multi_gpus=True)\n",
    "#             elif base_model == 'bert':\n",
    "#                 model = BertNER.from_pretrained('bert-base-cased', dataset_label_nums = dataset_label_nums, output_attentions=False, output_hidden_states=False, multi_gpus=True)\n",
    "#         if args.reinit:\n",
    "#             model = weights_init_custom(model)\n",
    "#         print(\"let's use \", torch.cuda.device_count(), \"GPUs!\")\n",
    "#         # torch.save(model, 'pretrained_models/lc_pretrained_190.pt')\n",
    "#         # exit(1)\n",
    "#         # model = torch.nn.DataParallel(model)\n",
    "#         model.to(device)\n",
    "#         model = torch.nn.DataParallel(model)\n",
    "#         # model = torch.nn.parallel.DistributedDataParallel(model,find_unused_parameters=True) # device_ids will include all GPU devices by default\n",
    "\n",
    "#         param_optimizer = list(model.named_parameters())\n",
    "\n",
    "#         no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n",
    "#         optimizer_grouped_parameters = [\n",
    "#             {'params': [p for n, p in param_optimizer\n",
    "#                         if not any(nd in n for nd in no_decay)], 'weight_decay': args.weight_decay},\n",
    "#             {'params': [p for n, p in param_optimizer\n",
    "#                         if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}]\n",
    "\n",
    "#         N_EPOCHS = args.epoch\n",
    "#         BATCH_SIZE = args.batch_size\n",
    "#         num_training_steps = N_EPOCHS * int(sum([len(sub_train_) for sub_train_ in processed_training_set]) / BATCH_SIZE)\n",
    "#         num_warmup_steps = int(args.warmup_proportion * num_training_steps)\n",
    "#         print(f\"num training steps: {num_training_steps}\")\n",
    "#         print(f\"num warmup steps: {num_warmup_steps}\")\n",
    "#         optimizer = AdamW(optimizer_grouped_parameters, lr=args.lr, correct_bias=True)\n",
    "#         # optimizer = Adam(optimizer_grouped_parameters, lr=args.lr, betas=(0.9,0.98), eps=1e-6)\n",
    "#         scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=num_warmup_steps, num_training_steps=num_training_steps)\n",
    "\n",
    "#         SOFT_KMEANS = args.soft_kmeans\n",
    "\n",
    "#         start_epoch = 0\n",
    "#         LOAD_CHECKPOINT = args.load_checkpoint\n",
    "#         if LOAD_CHECKPOINT:\n",
    "#             print(\"start loading checkpoint\")\n",
    "#             start_time = time.time()\n",
    "#             state = torch.load(args.load_model_name)\n",
    "\n",
    "#             model.load_state_dict(state['state_dict'])\n",
    "#             start_epoch = state['epoch']\n",
    "#             optimizer.load_state_dict(state['optimizer'])\n",
    "#             scheduler.load_state_dict(state['scheduler'])\n",
    "#             secs = int(time.time() - start_time)\n",
    "#             mins = secs / 60\n",
    "#             secs = secs % 60\n",
    "#             print(f\"loaded from checkpoint - learning rate: {optimizer.param_groups[0]['lr']:.9f} | time in {mins} minutes, {secs} seconds\")\n",
    "#             start_count_num = state['count_num']\n",
    "            \n",
    "    \n",
    "        \n",
    "#         result_dict = {'labels':['Epoch','Loss','Precision','Recall','F1-Score'],\n",
    "#                       'train_results': [],\n",
    "#                       'val_results': []}\n",
    "        \n",
    "#         train_results = []\n",
    "#         val_results = []\n",
    "        \n",
    "#         for epoch in range(start_epoch, N_EPOCHS):\n",
    "            \n",
    "            \n",
    "#             # Training process\n",
    "\n",
    "#             start_time = time.time()\n",
    "#             if args.load_checkpoint and epoch == start_epoch:\n",
    "#                 train_loss, microp, micror, microf1 = train_func(processed_training_set, epoch, tokenizer, train_label_sentence_dicts, soft_kmeans = SOFT_KMEANS, count_num = start_count_num)\n",
    "#             else:\n",
    "#                 train_loss, microp, micror, microf1 = train_func(processed_training_set, epoch, tokenizer, train_label_sentence_dicts, soft_kmeans = SOFT_KMEANS)\n",
    "            \n",
    "#             temp = [(train_loss), (microp * 100), (micror * 100), (microf1 * 100)]\n",
    "            \n",
    "#             # Keeping track of the results\n",
    "#             train_results.append(temp)\n",
    "#             print(f'\\tLoss: {train_loss:.4f}(train)\\t|\\tPrec: {microp * 100:.1f}%(train)\\t|\\tRecall: {micror * 100:.1f}%(train)\\t|\\tF1: {microf1 * 100:.1f}%(train)')\n",
    "\n",
    "            \n",
    "#             # Validation process\n",
    "    \n",
    "#             valid_loss, microp, micror, microf1, microp_per_type, micror_per_type, microf1_per_type = test(processed_val_set, epoch, val_label_sentence_dicts, soft_kmeans = SOFT_KMEANS)\n",
    "            \n",
    "#             temp = [(valid_loss), (microp * 100), (micror * 100), (microf1 * 100)]\n",
    "            \n",
    "#             # Keeping track of the results\n",
    "#             val_results.append(temp)\n",
    "#             print(f'\\tLoss: {valid_loss:.4f}(val)\\t|\\tPrec: {microp * 100:.1f}%(val)\\t|\\tRecall: {micror * 100:.1f}%(val)\\t|\\tF1: {microf1 * 100:.1f}%(val)')\n",
    "#             print(f'microp per type: {microp_per_type} \\nmicror_per_type: {micror_per_type} \\nmicrof1_per_type: {microf1_per_type}')\n",
    "            \n",
    "#             secs = int(time.time() - start_time)\n",
    "#             mins = secs / 60\n",
    "#             secs = secs % 60\n",
    "\n",
    "#             print('Epoch: %d' %(epoch + 1), \" | time in %d minutes, %d seconds\" %(mins, secs))\n",
    "            \n",
    "        \n",
    "#         total_f1_scores.append(microf1)\n",
    "    \n",
    "        \n",
    "#         # Save the dictionary as a JSON file\n",
    "#         string = args.dataset+\"_\"+args.model_save_name+\"_training.json\"\n",
    "#         results_path = os.path.join(\"results\", string)\n",
    "        \n",
    "#         result_dict['train_results'] = train_results\n",
    "#         result_dict['val_results'] = val_results\n",
    "#         with open(results_path, \"w\") as json_file:\n",
    "#             json.dump(result_dict, json_file)\n",
    "#         print(\"result saved!\")\n",
    "           \n",
    "        \n",
    "#         new_path = os.path.join(\"trained_model\", args.model_save_name+'.pt')\n",
    "#         torch.save(model, new_path)\n",
    "        \n",
    "#         new_path = os.path.join(\"trained_model\", args.model_save_name+'_dict.pt')\n",
    "#         torch.save(model.state_dict(), new_path)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#     # Printing some results\n",
    "#     print(f\"f1 scores: {total_f1_scores} \\n average f1 scores: {sum(total_f1_scores)/len(total_f1_scores)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0ca0efa1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0539beae",
   "metadata": {},
   "source": [
    "# Evaluation results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "93d1802e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20a78a95",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data(ner_tags, sents, tokenizer, label2id, max_sequence_len, base_model='roberta', lower=False, use_truecase=False):\n",
    "    logging.info(\"Start processing raw data!\")\n",
    "    sent_words, sent_wpcs, sent_tags = [], [], []\n",
    "    max_seq_len = 0\n",
    "    label_sentence_dict = defaultdict(list)\n",
    "    idx = 0\n",
    "    exceed_num = 0\n",
    "    good_sent = 0\n",
    "    bad_sent = 0\n",
    "    for k,line in enumerate(sents):\n",
    "        sent = line.strip()\n",
    "        # N.B. don't need to lower, since tokenizer does it automatically\n",
    "        # if lower:\n",
    "        #     sent = sent.lower()\n",
    "        words = sent.split(' ')\n",
    "        tags = ner_tags[k].strip().split()\n",
    "        # tags = [x if len(x.split('.')) == 1 else x.split('-')[0]+'-'+x.split('.')[-1] for x in tags]\n",
    "        # for t in tags:\n",
    "        #     if t not in label2id:\n",
    "        #         print(ner_tags[k])\n",
    "        #         print(t)\n",
    "        if len(tags) != len(words):\n",
    "            print(tags)\n",
    "            print(words)\n",
    "        assert len(tags) == len(words)\n",
    "        if use_truecase:\n",
    "            last_words = [x for x in words]\n",
    "            words = truecase_sentence(words)\n",
    "            if last_words != words:\n",
    "                print(last_words)\n",
    "                print(words)\n",
    "            sent = ' '.join(words)\n",
    "        if base_model == 'bert':\n",
    "            wpcs = [\"[CLS]\"]\n",
    "            wpcs.extend(tokenizer.tokenize(sent))\n",
    "            wpcs.append(\"[SEP]\")\n",
    "        elif base_model == 'roberta':\n",
    "            wpcs = ['<s>']\n",
    "            wpcs.extend(tokenizer.tokenize(sent))\n",
    "            wpcs.append('</s>')\n",
    "        if len(wpcs) > max_sequence_len:\n",
    "            exceed_num += 1\n",
    "        if len(wpcs) > max_seq_len:\n",
    "            max_seq_len = len(wpcs)\n",
    "        try:\n",
    "            # print(words)\n",
    "            # print(wpcs)\n",
    "            aligns = align_wpcs(words, wpcs, base_model = base_model, lower=lower) \n",
    "        except IndexError:\n",
    "            print(\"index error\")\n",
    "            print(words)\n",
    "            print(wpcs)\n",
    "            bad_sent += 1\n",
    "            continue\n",
    "        except AssertionError:\n",
    "            print(\"ignoring one\")\n",
    "            print(\"index error\")\n",
    "            print(words)\n",
    "            print(wpcs)\n",
    "            bad_sent += 1\n",
    "            continue\n",
    "        sent_wpcs.append(tokenizer.convert_tokens_to_ids(wpcs))\n",
    "        # sent_words.append(aligns)\n",
    "        # [(1, 2), (2, 4), (4, 5)]\n",
    "        sent_tag = [-100]\n",
    "        for i in range(len(aligns)):\n",
    "            sent_tag.extend([label2id[tags[i]] if j == 0 else -100 for j in range(aligns[i][1]-aligns[i][0])])\n",
    "            # if tags[i] == 'O':\n",
    "            #     sent_tag.extend([label2id[tags[i]] for j in range(aligns[i][1]-aligns[i][0])])\n",
    "            # else:\n",
    "            #     sent_tag.extend([label2id[tags[i]] if j == 0 else label2id['I-'+tags[i].split('-')[1]] for j in range(aligns[i][1]-aligns[i][0])])\n",
    "        sent_tag.append(-100)\n",
    "        assert len(sent_tag) == len(wpcs)\n",
    "        sent_tags.append(sent_tag)\n",
    "        good_sent += 1\n",
    "\n",
    "        # add dictionary: tag to sentence index\n",
    "        tags_set = set(tags)\n",
    "        all_O = True\n",
    "        for t in tags_set:\n",
    "            if label2id[t] != 0:\n",
    "                all_O = False\n",
    "                # if k == 0:\n",
    "                #     print(t)\n",
    "                label_sentence_dict[label2id[t]].append(idx)\n",
    "        if all_O:\n",
    "            label_sentence_dict[0].append(idx)\n",
    "        # if k == 0:\n",
    "        #     print(label_sentence_dict)\n",
    "        idx += 1\n",
    "    # out_file = open('label_sentence_dict','w')\n",
    "    # json.dump(label_sentence_dict, out_file)\n",
    "\n",
    "    logging.info(f\"max seq length: {max_seq_len}\")\n",
    "    logging.info(f\"truncate ratio: {exceed_num/len(sents)}\")\n",
    "    logging.info(f\"good sent: {good_sent} bad sent: {bad_sent}\")\n",
    "    return sent_tags, sent_wpcs, label_sentence_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "18543dd9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 'O', 1: 'B-Company_Name', 2: 'I-Company_Name', 3: 'B-Internal_Organization', 4: 'I-Internal_Organization', 5: 'B-Software_Name', 6: 'I-Software_Name', 7: 'B-Userbase_Information', 8: 'I-Userbase_Information', 9: 'B-Software_Purpose', 10: 'I-Software_Purpose', 11: 'B-Development_Scalability', 12: 'I-Development_Scalability', 13: 'B-Transaction_Scalability', 14: 'I-Transaction_Scalability', 15: 'B-Data_Scalability', 16: 'I-Data_Scalability'}\n",
      "{'O': 0, 'B-Company_Name': 1, 'I-Company_Name': 2, 'B-Internal_Organization': 3, 'I-Internal_Organization': 4, 'B-Software_Name': 5, 'I-Software_Name': 6, 'B-Userbase_Information': 7, 'I-Userbase_Information': 8, 'B-Software_Purpose': 9, 'I-Software_Purpose': 10, 'B-Development_Scalability': 11, 'I-Development_Scalability': 12, 'B-Transaction_Scalability': 13, 'I-Transaction_Scalability': 14, 'B-Data_Scalability': 15, 'I-Data_Scalability': 16}\n"
     ]
    }
   ],
   "source": [
    "# Data preparation\n",
    "train_text = os.path.join(args.datapath, args.dataset, args.train_text) \n",
    "train_ner = os.path.join(args.datapath, args.dataset, args.train_ner) \n",
    "test_text = os.path.join(args.datapath, args.dataset, args.test_text)\n",
    "test_ner = os.path.join(args.datapath, args.dataset, args.test_ner) \n",
    "\n",
    "with open(train_ner, encoding='utf-8') as fner, open(train_text, encoding='utf-8') as f:\n",
    "    train_ner_tags, train_words = fner.readlines(), f.readlines() \n",
    "with open(test_ner, encoding='utf-8') as fner, open(test_text, encoding='utf-8') as f:\n",
    "    test_ner_tags, test_words = fner.readlines(), f.readlines()     \n",
    "    \n",
    "# Removing the first 20% of test set because they were used in the validation set\n",
    "split_index = int(len(test_ner_tags) * 0.2)\n",
    "\n",
    "test_ner_tags = test_ner_tags[split_index:]\n",
    "test_words = test_words[split_index:]\n",
    "\n",
    "\n",
    "# The tokenizer for roberta\n",
    "tokenizer = RobertaTokenizer.from_pretrained('roberta-base')    \n",
    "    \n",
    "# Getting the labell ids\n",
    "label2ids, id2labels = [], []\n",
    "processed_training_set, train_label_sentence_dicts, processed_test_set, test_label_sentence_dicts = [], [], [], []\n",
    "\n",
    "label2id, id2label = get_label_dict([train_ner_tags, test_ner_tags])\n",
    "\n",
    "label2ids.append(label2id)\n",
    "id2labels.append(id2label)\n",
    "\n",
    "#Keeping track of the unprocessed data\n",
    "unprocessed_test_ner_tags = test_ner_tags\n",
    "unprocessed_test_words = test_words\n",
    "\n",
    "\n",
    "# Processesing data to input into model\n",
    "max_seq_len = args.max_seq_len\n",
    "train_ner_tags, train_words, train_label_sentence_dict = process_data(train_ner_tags, train_words, tokenizer, label2id, max_seq_len,base_model=args.base_model,use_truecase=args.use_truecase)\n",
    "test_ner_tags, test_words, test_label_sentence_dict = process_data(test_ner_tags, test_words, tokenizer, label2id, max_seq_len,base_model=args.base_model,use_truecase=args.use_truecase)\n",
    "\n",
    "\n",
    "sub_train_ = [[train_words[i], train_ner_tags[i]] for i in range(len(train_ner_tags))]\n",
    "sub_valid_ = [[test_words[i], test_ner_tags[i]] for i in range(len(test_ner_tags))] \n",
    "\n",
    "train_label_sentence_dicts.append(train_label_sentence_dict)\n",
    "test_label_sentence_dicts.append(test_label_sentence_dict)\n",
    "\n",
    "# print([(x,len(train_label_sentence_dict[x])) for x in train_label_sentence_dict])\n",
    "processed_training_set.append(sub_train_) \n",
    "processed_test_set.append(sub_valid_) \n",
    "\n",
    "\n",
    "dataset_label_nums = [len(x) for x in label2ids]\n",
    "train_num_data_point = sum([len(sub_train_) for sub_train_ in processed_training_set])\n",
    "test_num_data_point = sum([len(sub_train_) for sub_train_ in processed_test_set])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69b3a63f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9534d71e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaNER: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight']\n",
      "- This IS expected if you are initializing RobertaNER from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing RobertaNER from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaNER were not initialized from the model checkpoint at roberta-base and are newly initialized: ['background', 'classifier.weight', 'classifier.bias', 'classifiers.0.weight', 'classifiers.0.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Importing the fine-tuned model\n",
    "new_model = RobertaNER.from_pretrained('roberta-base', dataset_label_nums=dataset_label_nums, output_attentions=False, output_hidden_states=False, multi_gpus=True)\n",
    "new_model = torch.nn.DataParallel(new_model)\n",
    "i = 0\n",
    "new_model.load_state_dict(torch.load(os.path.join(\"trained_model\",f\"FS_train_dataset{i}_finetuned_model_dict.pt\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "7d730e40",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predictions(data_):\n",
    "    dataset_chosen = []\n",
    "    data = []\n",
    "    for i,d in enumerate(data_):\n",
    "        one_dataset = [generate_batch(d)]\n",
    "        data.extend(one_dataset)\n",
    "        dataset_chosen.extend([i for x in range(len(one_dataset))])\n",
    "        \n",
    "    idx = 0\n",
    "    f1ss = []\n",
    "    pss = []\n",
    "    rss = []\n",
    "    \n",
    "    \n",
    "    new_model.eval()\n",
    "    \n",
    "    for j, (text, attention_mask, cls, orig_len) in enumerate(data):\n",
    "        with torch.no_grad():\n",
    "            text_1, attention_mask_1, cls_1 = text.to(device), attention_mask.to(device).to(device), cls.to(device)\n",
    "            loss, outputs = new_model(text_1, attention_mask=attention_mask_1, labels=cls_1, dataset = dataset_chosen[j])\n",
    "        preds = [[id2label[int(x)] for j,x in enumerate(y[1:orig_len[i]-1]) if int(cls[i][j + 1]) != -100] for i,y in enumerate(outputs)]\n",
    "        gold = [[id2label[int(x)] for x in y[1:orig_len[i]-1] if int(x) != -100] for i,y in enumerate(cls)]\n",
    "    \n",
    "    return preds, gold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "90b51660",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "pred, original = get_predictions(processed_test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "290bdf1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(processed_test_set[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "2fc2aa7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "120"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "b8aa10b1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "120"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(original)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9b57cf7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_entities(labels):\n",
    "    entities = {}\n",
    "    start_idx = None\n",
    "\n",
    "    for idx, label in enumerate(labels):\n",
    "        if label.startswith('B-'):\n",
    "            # Found the start of an entity\n",
    "            if start_idx is not None:\n",
    "                # Add the previous entity to the dictionary\n",
    "                entity = labels[start_idx].split('-')[1]\n",
    "                entities[entity] = entities.get(entity,[]) + [list(range(start_idx, idx))]\n",
    "            \n",
    "            start_idx = idx\n",
    "        elif label.startswith('I-'):\n",
    "            # Continue the current entity\n",
    "            if start_idx is None:\n",
    "                start_idx = idx\n",
    "        else:\n",
    "            # End of entity\n",
    "            if start_idx is not None:\n",
    "                entity = labels[start_idx].split('-')[1]\n",
    "                entities[entity] = entities.get(entity,[]) + [list(range(start_idx, idx))]\n",
    "                start_idx = None\n",
    "\n",
    "    # Check if there's an entity that spans till the end of the list\n",
    "    if start_idx is not None:\n",
    "        entity = labels[start_idx].split('-')[1]\n",
    "        entities[entity] = entities.get(entity,[]) + [list(range(start_idx, len(labels)))]\n",
    "\n",
    "    return entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "372410f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_to_labels = {'B-Transaction_Scalability': 'Transaction_Scalability',\n",
    " 'I-Transaction_Scalability': 'Transaction_Scalability',\n",
    " 'I-Software_Purpose': 'Software_Purpose',\n",
    " 'B-Development_Scalability': 'Development_Scalability',\n",
    " 'I-Development_Scalability': 'Development_Scalability',\n",
    " 'B-Userbase_Information': 'Userbase_Information',\n",
    " 'I-Userbase_Information': 'Userbase_Information',\n",
    " 'B-Data_Scalability': 'Data_Scalability',\n",
    " 'B-Software_Purpose': 'Software_Purpose',\n",
    " 'B-Internal_Organization': 'Internal_Organization',\n",
    " 'I-Internal_Organization': 'Internal_Organization',\n",
    " 'I-Data_Scalability': 'Data_Scalability',\n",
    " 'B-Software_Name': 'Software_Name',\n",
    " 'I-Software_Name': 'Software_Name',\n",
    " 'B-Company_Name': 'Company_Name',\n",
    " 'I-Company_Name': 'Company_Name',\n",
    " 'O': 'O'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fb78a6a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Score_Container:\n",
    "    def __init__(self):\n",
    "        self.tp = 0\n",
    "        self.tn = 0\n",
    "        self.fp = 0\n",
    "        self.fn = 0\n",
    "        self.fn2 = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6e3e121c",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_by_entity = {}\n",
    "for i in set(labels_to_labels.values()):\n",
    "    scores_by_entity[i] = Score_Container()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a61f0c8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_scores(ner_result, correct_tags):\n",
    "    tp, tn, fp, fn = 0, 0, 0, 0\n",
    "    \n",
    "    # Making a flat list of all the indexes that contain an entity\n",
    "    correct_tags_positions = []\n",
    "    for n in correct_tags:\n",
    "        flat_list = [item for sublist in correct_tags[n] for item in sublist]\n",
    "        correct_tags_positions += flat_list\n",
    "    \n",
    "    ner_result_positions = []\n",
    "    for n in ner_result:\n",
    "        flat_list = [item for sublist in ner_result[n] for item in sublist]\n",
    "        ner_result_positions += flat_list\n",
    "    \n",
    "    for n in ner_result:\n",
    "        for sublist1 in ner_result[n]:\n",
    "            if n in correct_tags:\n",
    "                found = False\n",
    "                for sublist2 in correct_tags[n]:\n",
    "                    if set(sublist1) & set(sublist2):\n",
    "                        # Assigns the correct entity in the text\n",
    "                        tp += 1\n",
    "                        found = True\n",
    "                        scores_by_entity[n].tp += 1\n",
    "                if not found and set(sublist1) & set(correct_tags_positions): \n",
    "                    # Fails to assign the correct entity but overlaps with another entity\n",
    "                    fn += 1\n",
    "                    scores_by_entity[n].fn += 1\n",
    "                    found = True\n",
    "                if not found:\n",
    "                    # Assigned a label to a non-entity\n",
    "                    fp += 1\n",
    "                    scores_by_entity[n].fp += 1\n",
    "            else:\n",
    "                found = False\n",
    "                if set(sublist1) & set(correct_tags_positions): \n",
    "                    # Fails to assign the correct entity but overlaps with another entity\n",
    "                    fn += 1\n",
    "                    scores_by_entity[n].fn += 1\n",
    "                    found = True\n",
    "                if not found:\n",
    "                    # Assigned a label to a non-entity\n",
    "                    fp += 1\n",
    "                    scores_by_entity[n].fp += 1\n",
    "                \n",
    "                \n",
    "    \n",
    "    for n in correct_tags:\n",
    "        for sublist2 in correct_tags[n]:\n",
    "            # Fails to assign an entity at all\n",
    "            if not set(sublist2) & set(ner_result_positions):\n",
    "#                 print(sublist2)\n",
    "#                 print(ner_result_positions)\n",
    "#                 print(\"+++++++++++++++++++\")\n",
    "                fn += 1\n",
    "                scores_by_entity[n].fn2 += 1\n",
    "    \n",
    "#     print(tp, tn, fp, fn)\n",
    "    return tp, tn, fp, fn           \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9421a87f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed Time: 0.004999637603759766 seconds\n"
     ]
    }
   ],
   "source": [
    "true_positives, true_negatives, false_positives, false_negatives = 0, 0, 0, 0\n",
    "start_time = time.time()\n",
    "for i in range(len(pred)):\n",
    "    ner_result = find_entities(pred[i])\n",
    "    correct_tags = find_entities(original[i])\n",
    "    tp, tn, fp, fn = get_scores(ner_result, correct_tags)\n",
    "    true_positives += tp\n",
    "    true_negatives += tn\n",
    "    false_positives += fp\n",
    "    false_negatives += fn\n",
    "    \n",
    "#     print(ner_result)\n",
    "#     print(correct_tags)\n",
    "#     if i > 20:\n",
    "#         break\n",
    "\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "\n",
    "# Print the elapsed time\n",
    "print(f\"Elapsed Time: {elapsed_time} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e221dd28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120\n",
      "120\n",
      "120\n",
      "120\n"
     ]
    }
   ],
   "source": [
    "print(len(pred))\n",
    "print(len(original))\n",
    "print(len(unprocessed_test_words))\n",
    "print(len(unprocessed_test_ner_tags))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6f7d7f4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking if the prediction and the original fed into the model are the same before we place it the evaluation\n",
    "for i in range(len(pred)):\n",
    "#     print(pred[i])\n",
    "#     print(original[i])\n",
    "#     print(unprocessed_test_words[i])\n",
    "#     print(unprocessed_test_ner_tags[i])\n",
    "    if original[i] != unprocessed_test_ner_tags[i].split():\n",
    "        print(original[i])\n",
    "        print(unprocessed_test_ner_tags[i].split())\n",
    "        print(\"-----------------------\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "fcb3a457",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------+------+------+------+------+-------+\n",
      "|                         |   tp |   tn |   fp |   fn |   fn2 |\n",
      "+=========================+======+======+======+======+=======+\n",
      "| Development_Scalability |  142 |    0 |   37 |    5 |    30 |\n",
      "+-------------------------+------+------+------+------+-------+\n",
      "| Software_Purpose        |   18 |    0 |   17 |    3 |    14 |\n",
      "+-------------------------+------+------+------+------+-------+\n",
      "| Software_Name           |   48 |    0 |   12 |    5 |     6 |\n",
      "+-------------------------+------+------+------+------+-------+\n",
      "| Company_Name            |   16 |    0 |    5 |    2 |     2 |\n",
      "+-------------------------+------+------+------+------+-------+\n",
      "| Data_Scalability        |   49 |    0 |    7 |    8 |     7 |\n",
      "+-------------------------+------+------+------+------+-------+\n",
      "| O                       |    0 |    0 |    0 |    0 |     0 |\n",
      "+-------------------------+------+------+------+------+-------+\n",
      "| Internal_Organization   |   54 |    0 |    8 |    1 |     7 |\n",
      "+-------------------------+------+------+------+------+-------+\n",
      "| Userbase_Information    |   49 |    0 |    5 |    5 |     7 |\n",
      "+-------------------------+------+------+------+------+-------+\n",
      "| Transaction_Scalability |   55 |    0 |   21 |    6 |    10 |\n",
      "+-------------------------+------+------+------+------+-------+\n"
     ]
    }
   ],
   "source": [
    "from tabulate import tabulate\n",
    "index = list(scores_by_entity.keys())\n",
    "columns = ['tp','tn','fp','fn','fn2']\n",
    "\n",
    "data = []\n",
    "for i in scores_by_entity:\n",
    "    temp = [scores_by_entity[i].tp,scores_by_entity[i].tn,scores_by_entity[i].fp,scores_by_entity[i].fn,scores_by_entity[i].fn2]\n",
    "    data.append(temp)\n",
    "\n",
    "# Create a NumPy array from the data\n",
    "data_array = np.array(data)\n",
    "\n",
    "# Create a table using the tabulate function\n",
    "table = tabulate(data_array, headers=columns, showindex=index, tablefmt=\"grid\")\n",
    "\n",
    "print(table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4b51ce51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "431 0 112 118\n"
     ]
    }
   ],
   "source": [
    "print(true_positives, true_negatives, false_positives, false_negatives)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f747f9ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7937384898710865"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precision = true_positives/(true_positives+false_positives)\n",
    "precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2f664921",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.785063752276867"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recall = true_positives/(true_positives+false_negatives)\n",
    "recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e9affe82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7893772893772893"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1 = (true_positives)/(true_positives+0.5*(false_positives+false_negatives))\n",
    "f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "723df4fb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b35624ee",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
